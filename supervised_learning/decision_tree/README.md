Decision Tree & Random Forest

Resources

Read or watch:

(1) Rokach and Maimon (2002) : Top-down induction of decision trees classifiers : a survey
(2) Ho et al. (1995) : Random Decision Forests
(3) Fei et al. (2008) : Isolation forests
(4) Gini and Entropy clearly explained : Handling Continuous features in Decision Trees
(5) Abspoel and al. (2021) : Secure training of decision trees with continuous attributes
(6) Threshold Split Selection Algorithm for Continuous Features in Decision Tree
(7) Splitting Continuous Attribute using Gini Index in Decision Tree
(8) How to handle Continuous Valued Attributes in Decision Tree
(9) Decision Tree problem based on the Continuous-valued attribute
(10) How to Implement Decision Trees in Python using Scikit-Learn(sklearn)
(11)Matching and Prediction on the Principle of Biological Classification by William A. Belson

Notes
This project aims to implement decision trees from scratch. It is important for engineers to understand how the tools we use are built for two reasons. First, it gives us confidence in our skills. Second, it helps us when we need to build our own tools to solve unsolved problems.
The first three references point to historical papers where the concepts were first studied.
References 4 to 9 can help if you feel you need some more explanation about the way we split nodes.
William A. Belson is usually credited for the invention of decision trees (read reference 11).
Despite our efforts to make it efficient, we cannot compete with Sklearn’s implementations (since they are done in C). In real life, it is thus recommended to use Sklearn’s tools.
In this regard, it is warmly recommended to watch the video referenced as (10) above. It shows how to use Sklearn’s decision trees and insists on the methodology.
